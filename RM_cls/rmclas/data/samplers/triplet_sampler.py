# !/user/bin/env python
# coding=utf-8
"""
@project : RM_clas
@author  : lmliu
@contact : lmliu@streamax.com
#@file   : triplet_sampler.py.py
#@time   : 2020-12-22 20:32:28
"""

import copy
import itertools
from collections import defaultdict
from typing import Optional

import numpy as np
from torch.utils.data.sampler import Sampler

from rmclas.utils import comm


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


class NaiveIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, label).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, num_classes: int,
                 seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_labels_per_batch = mini_batch_size // self.num_instances

        assert self.num_labels_per_batch <= num_classes, \
            "label nums per batch should less than class nums," \
            " please increase num_instances at least {}".format(mini_batch_size/num_classes)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.label_index = defaultdict(list)

        for index, info in enumerate(data_source):
            label = info[1]
            self.label_index[label].append(index)

        self.labels = sorted(list(self.label_index.keys()))
        self.num_identities = len(self.labels)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            avl_labels = copy.deepcopy(self.labels)
            batch_idxs_dict = {}

            batch_indices = []
            while len(avl_labels) >= self.num_labels_per_batch:
                selected_labels = np.random.choice(avl_labels, self.num_labels_per_batch, replace=False).tolist()
                for label in selected_labels:
                    # Register label in batch_idxs_dict if not
                    if label not in batch_idxs_dict:
                        idxs = copy.deepcopy(self.label_index[label])
                        if len(idxs) < self.num_instances:
                            idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                        np.random.shuffle(idxs)
                        batch_idxs_dict[label] = idxs

                    avl_idxs = batch_idxs_dict[label]
                    for _ in range(self.num_instances):
                        batch_indices.append(avl_idxs.pop(0))

                    if len(avl_idxs) < self.num_instances: avl_labels.remove(label)

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []
